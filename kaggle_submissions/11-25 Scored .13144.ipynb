{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso + 2 OLS + RF + GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "# data manipulation/viz\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling setups\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm \n",
    "\n",
    "# tree modeling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# other\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# turn off the df['col'] = x assignment warning\n",
    "#pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional directory set-up\n",
    "train = pd.read_csv(\"../../housing_data/train.csv\")\n",
    "test = pd.read_csv(\"../../housing_data/test.csv\")\n",
    "sample = pd.read_csv(\"../../housing_data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train =pd.read_csv('C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/train.csv')\n",
    "#  test = pd.read_csv(\"C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/test.csv\")\n",
    "#  sample = pd.read_csv(\"C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data clean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_clean(df):\n",
    "    \n",
    "    # some vars are just too missing so I remove the field\n",
    "    df = df.drop(columns = [\"PoolQC\", \"MiscFeature\"])\n",
    "\n",
    "    # replace some numeric vars w/ median\n",
    "    median_replace_vars = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
    "    for var in median_replace_vars:\n",
    "        df[var].fillna(df[var].median(), inplace = True)\n",
    "    \n",
    "    # replace some num vars w/ 0\n",
    "    zero_replace_vars = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n",
    "    for var in zero_replace_vars:\n",
    "        df[var].fillna(0, inplace = True)\n",
    "    \n",
    "    # replace some cat vars w/ most freq value \n",
    "    df['MasVnrType'].fillna('None', inplace = True)\n",
    "    df['Electrical'].fillna('SBrkr', inplace = True)\n",
    "    df['MSZoning'].fillna('RL', inplace = True)\n",
    "    df['SaleType'].fillna('WD', inplace = True)\n",
    "    df['Utilities'].fillna('AllPub', inplace = True)\n",
    "    df['KitchenQual'].fillna('TA', inplace = True)\n",
    "    df['Functional'].fillna('Typ', inplace = True)\n",
    "\n",
    "    # other cat vars just put missing if there isn't a glaring most popular category\n",
    "    replace_missing_vars = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Exterior1st', \n",
    "        'Exterior2nd', 'FireplaceQu']\n",
    "    for var in replace_missing_vars:\n",
    "        df[var].fillna(\"Missing\", inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function for some standard feature engineering to use in all models\n",
    "def standard_feature_eng(df, test_data = False):\n",
    "    '''Input either the training or test data. \n",
    "    2nd arg set to True if it's the testing data. That way we ignore the final log transformation on sale price'''\n",
    "\n",
    "    # num features to just binarize b/c few houses have the feature\n",
    "    df[\"SwimmingPool\"] = df['PoolArea'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"3SsnPorch\"] = df['3SsnPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"ScreenPorch\"] = df['ScreenPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    # re-factoring vars:\n",
    "    # group the irregularities into 2 factor levels\n",
    "    df['LotShape'] = df['LotShape'].map({'Reg': 'Reg', 'IR1': 'Reg', 'IR2': 'Irreg', 'IR3': 'Irreg'})\n",
    "\n",
    "    # simplifying MSSubClass because we have the year built in another feature\n",
    "    df['MSSubClass'] = df['MSSubClass'].map(lambda x: \n",
    "        \"1_story\"   if (x in (20, 30, 40, 120)) else(\n",
    "        \"1.5_story\" if (x in (45, 50, 150)) else(\n",
    "        \"2_story\"   if (x in (60, 70, 75, 160, 180, 190)) else(\n",
    "        \"split\"     if (x in (80, 85)) else(\n",
    "        \"duplex\"    if (x ==90) else(\n",
    "        \"other\"))))))\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"object\")\n",
    "\n",
    "    # simplifying more vars\n",
    "    # electrical:\n",
    "    df['Electrical'] = df['Electrical'].map(lambda x: \"SBrkr\" if x == \"SBrkr\" else \"Fuse\")\n",
    "    # exterior:\n",
    "    df['Exterior'] = df['Exterior1st'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Expensive\" if (x in (\"VinylSd\", \"CemntBd\", \"Stone\", \"ImStucc\")) else(\n",
    "        \"Cheap\" if (x in (\"BrkComm\", \"AsphShn\", \"CBlock\", \"AsbShng\")) else(\n",
    "        \"Moderate\")))\n",
    "    df = df.drop(columns=['Exterior1st', 'Exterior2nd'])\n",
    "    # garage\n",
    "    df['GarageQual'] = df['GarageQual'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Good\" if (x in (\"Ex\", \"Gd\")) else(\n",
    "        \"Medium\" if (x in (\"TA\")) else(\n",
    "        \"Bad\")))\n",
    "    df['Heating'] = df['Heating'].map(lambda x: \"Gas\" if x in (\"GasA\", \"GasW\") else \"Other\")\n",
    "\n",
    "    # deciding to drop a few features for various reasons\n",
    "    vars_to_drop = [\n",
    "        # not much variation:\n",
    "        \"LowQualFinSF\", \n",
    "        \"LandSlope\", \n",
    "        \"MiscVal\", \n",
    "        \"RoofMatl\",\n",
    "        \"Condition2\",\n",
    "        #\"KitchenAbvGr\" # hardly any variation. But, Deva included in lm's so including it now.\n",
    "        \"PoolArea\", # binarized above\n",
    "        \"Utilities\", # only 1 obs in training data different from regular\n",
    "        \"HouseStyle\" # already explained in other vars\n",
    "        ]\n",
    "    df.drop(columns=vars_to_drop, inplace=True) \n",
    "\n",
    "    # adding a remodeled feature\n",
    "    df['Remodeled'] = (df.YearRemodAdd-df.YearBuilt) == 0\n",
    "\n",
    "    # total inside area will be a sum of 1st and 2nd floor sq ft\n",
    "    df['Total_Inside_Area'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df.drop(columns = ['1stFlrSF', '2ndFlrSF', 'GrLivArea'], inplace = True)\n",
    "\n",
    "    # simplify the bathrooms variable\n",
    "    df['Bathrooms'] = df.BsmtFullBath + 0.5*df.BsmtHalfBath + df.FullBath + 0.5*df.HalfBath\n",
    "    df.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], inplace = True)\n",
    "\n",
    "    # get log of sale price which will be our actual response variable\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df['LogSalePrice'] = np.log(df.SalePrice)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS with 16 features predicting regular sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.859\n",
      "Score on testing data: -17.901\n",
      "Log RMSE on training data: 0.141\n",
      "Log RMSE on testing data: 0.388\n"
     ]
    }
   ],
   "source": [
    "def lm_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    lm_vars = ['LotArea', 'Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'YearRemodAdd', \n",
    "              'BsmtCond', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'YrSold', \n",
    "             'MoSold', 'Remodeled', 'Total_Inside_Area', 'Bathrooms', 'GarageCars', 'BsmtFinSF1']\n",
    "#, 'YearRemodAdd',\n",
    "        #'LotFrontage','WoodDeckSF', 'GarageArea'\n",
    "    \n",
    "    df = pd.get_dummies(df[lm_vars], \n",
    "        columns = ['Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'BsmtCond','KitchenQual'], \n",
    "        drop_first=True)\n",
    "    \n",
    "    \n",
    "    #lm_vars = ['LotArea', 'OverallQual', 'OverallCond', 'YearRemodAdd', \n",
    "    #          'BsmtCond', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'YrSold', \n",
    "    #          'MoSold', 'Remodeled', 'Total_Inside_Area', 'Bathrooms', 'GarageCars', 'BsmtFinSF1', 'YearRemodAdd',\n",
    "    #    'LotFrontage','WoodDeckSF', 'GarageArea']\n",
    "    \n",
    "    #df = pd.get_dummies(df[lm_vars], \n",
    "    #    columns = [ 'OverallQual', 'OverallCond', 'BsmtCond','KitchenQual'], \n",
    "    #    drop_first=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# data setups\n",
    "X_train = lm_df_clean(train)\n",
    "X_test = lm_df_clean(test, test_data=True)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "# fit to train data\n",
    "lr_1 = LinearRegression(fit_intercept=True).fit(X_train, Y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lr_1.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lr_1.score(X_test ,Y_test)))\n",
    "\n",
    "yhat_train = lr_1.predict(X_train)\n",
    "yhat_test = lr_1.predict(X_test)\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((np.log(yhat_train) - np.log(Y_train))**2))**.5\n",
    "rmse_test = (np.mean((np.log(yhat_test) - np.log(Y_test))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS with 1 variable (overall quality) predicting regular sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.626\n",
      "Score on testing data: -14.882\n",
      "Log RMSE on training data: 0.811\n",
      "Log RMSE on testing data: 1.354\n"
     ]
    }
   ],
   "source": [
    "# data setup function\n",
    "def lm_overall_quality_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "    df = df.loc[:, ['OverallQual']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# data setups\n",
    "X_train = lm_overall_quality_df_clean(train)\n",
    "X_test = lm_overall_quality_df_clean(test, test_data=True)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "# fit to train data\n",
    "lr_overall_quality = LinearRegression(fit_intercept=True).fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lr_overall_quality.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lr_overall_quality.score(X_test ,Y_test)))\n",
    "\n",
    "yhat_train = lr_overall_quality.predict(X_train)\n",
    "yhat_test = lr_overall_quality.predict(X_test)\n",
    "\n",
    "# set negative values to 0.1\n",
    "yhat_train = np.array([0.1 if i < 0 else i for i in yhat_train])\n",
    "yhat_test = [0.1 if i < 0 else i for i in yhat_test]\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((np.log(yhat_train) - np.log(Y_train))**2))**.5\n",
    "rmse_test = (np.mean((np.log(yhat_test) - np.log(Y_test))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso predicting log sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.919\n",
      "Score on testing data: -17.746\n",
      "Log RMSE on training data: 0.113\n",
      "Log RMSE on testing data: 0.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tj/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.389841448502231, tolerance: 0.023280065898865106\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# first build one hot encoder based on the training data\n",
    "train_lasso = standard_feature_eng(na_clean(train))\n",
    "enc_lasso = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc_lasso.fit(train_lasso.select_dtypes(include=[\"object\"]))\n",
    "one_hot_columns = pd.get_dummies(train_lasso.select_dtypes(include=[\"object\"])).columns\n",
    "# will use this encoder in the function below\n",
    "\n",
    "# data setup function\n",
    "def lasso_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    # one hot encode using encoder above\n",
    "    categorical_cols = pd.DataFrame(enc_lasso.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_columns\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "\n",
    "    \n",
    "    # log transformations\n",
    "    #df[\"GrLivArea\"] = np.log(df[\"GrLivArea\"])\n",
    "    #df.Total_Inside_Area = np.log(df.Total_Inside_Area)\n",
    "\n",
    "    \n",
    "    # select only vars needed\n",
    "    if test_data:\n",
    "        df = df.drop(columns=[\"Id\"])\n",
    "    else:\n",
    "        df = df.drop(columns=[\"Id\"])\n",
    "        df['SalePrice'] = np.log(df['SalePrice'])\n",
    "        #df = df[[\"GrLivArea\",\"OverallQual\", \"SalePrice\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# data setups\n",
    "X_train = lasso_df_clean(train)\n",
    "X_test = lasso_df_clean(test, test_data=True)\n",
    "Y_train = X_train.SalePrice\n",
    "X_train = X_train.drop(columns=['LogSalePrice', 'SalePrice'])\n",
    "Y_test = np.log(sample.SalePrice)\n",
    "\n",
    "    \n",
    "# fit to train data\n",
    "lasso_fit = Lasso(alpha=0.000001).fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# Evaluate performance\n",
    "yhat_train = lasso_fit.predict(X_train)\n",
    "yhat_test = lasso_fit.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lasso_fit.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lasso_fit.score(X_test ,Y_test)))\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest to predict log sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.983\n",
      "Score on testing data: -15.401\n",
      "Log RMSE on training data: 0.052\n",
      "Log RMSE on testing data: 0.365\n"
     ]
    }
   ],
   "source": [
    "# first build one hot encoder based on the training data\n",
    "train_rf = standard_feature_eng(na_clean(train))\n",
    "enc_rf = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc_rf.fit(train_rf.select_dtypes(include=[\"object\"]))\n",
    "one_hot_columns = pd.get_dummies(train_rf.select_dtypes(include=[\"object\"])).columns\n",
    "# will use this encoder in the function below\n",
    "\n",
    "# Random forest data clean function\n",
    "def rf_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    # one hot encode using encoder above\n",
    "    categorical_cols = pd.DataFrame(enc_rf.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_columns\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "    \n",
    "    # DO FEATURE ENGINEERING HERE\n",
    "    # drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "df_rf = rf_df_clean(train)\n",
    "df_rf_test = rf_df_clean(test, test_data=True)\n",
    "\n",
    "# run model on best parameters\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators = 900,\n",
    "    max_depth = 25,\n",
    "    max_features = 'auto',\n",
    "    min_samples_split = 2,  \n",
    "    bootstrap = True, \n",
    "    )\n",
    "\n",
    "# fit the model\n",
    "rf_reg = rf_reg.fit(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_rf.LogSalePrice)\n",
    "\n",
    "# Evaluate performance\n",
    "yhat_train = rf_reg.predict(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']))\n",
    "yhat_test = rf_reg.predict(df_rf_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(rf_reg.score(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_rf.LogSalePrice)))\n",
    "print(\"Score on testing data: {:.3f}\".format(rf_reg.score(df_rf_test, np.log(sample.SalePrice))))\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost to predict log sale price (no longer using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first build one hot encoder based on the training data\n",
    "# train_xgb = standard_feature_eng(na_clean(train))\n",
    "# enc_xgb = OneHotEncoder(handle_unknown = 'ignore')\n",
    "# enc_xgb.fit(train_xgb.select_dtypes(include=[\"object\"]))\n",
    "# one_hot_columns_xgb = pd.get_dummies(train_xgb.select_dtypes(include=[\"object\"])).columns\n",
    "# # will use this encoder in the function below\n",
    "\n",
    "# # xgboost data clean function\n",
    "# def xgb_df_clean(df, test_data = False):\n",
    "\n",
    "#     # first run standard data cleaning steps\n",
    "#     df = na_clean(df)\n",
    "#     df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "#     # one hot encode using encoder above\n",
    "#     categorical_cols = pd.DataFrame(enc_xgb.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "#     categorical_cols.columns = one_hot_columns_xgb\n",
    "#     df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "    \n",
    "#     # DO MORE FEATURE ENGINEERING HERE LATER\n",
    "#     df = df.drop(columns = ['Id'])\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# # preprocess the data\n",
    "# df_xgb = xgb_df_clean(train)\n",
    "# df_xgb_test = xgb_df_clean(test, test_data=True)\n",
    "\n",
    "# # get X feature names\n",
    "# xgb_cols = np.array(df_xgb.drop(columns = [\"SalePrice\", 'LogSalePrice']).columns)\n",
    "\n",
    "# # convert data to DMatrix format\n",
    "# dmat_train = xgb.DMatrix(df_xgb.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_xgb['LogSalePrice'], feature_names=xgb_cols)\n",
    "# dmat_test = xgb.DMatrix(df_xgb_test, np.log(sample.SalePrice), feature_names=xgb_cols)\n",
    "\n",
    "# # train model\n",
    "# booster = xgb.train({\n",
    "\n",
    "#     \"booster\": \"gbtree\", \n",
    "#     \"max_depth\": 30, \n",
    "#     \"eta\": .4, \n",
    "#     \"gamma\": .01, \n",
    "#     \"subsample\": 0.6,\n",
    "#     \"lambda\": .7, \n",
    "#     \"alpha\": 0, \n",
    "#     \"max_bin\": 256, \n",
    "#     \"colsample_bytree\": .7, # proportion of features\n",
    "#     \"eval_metric\": \"rmse\", \n",
    "#     \"objective\": \"reg:squarederror\"\n",
    "#     },\n",
    "\n",
    "#     dmat_train,\n",
    "#     evals=[(dmat_train, \"train\"), (dmat_test, \"test\")] \n",
    "# )\n",
    "\n",
    "# # Evaluate performance\n",
    "# yhat_train = booster.predict(dmat_train)\n",
    "# yhat_test = booster.predict(dmat_test)\n",
    "\n",
    "# # # rmse\n",
    "# rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "# rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "# print(\"\\nLog RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "# print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLM predicting regular sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              SalePrice   No. Observations:                 1460\n",
      "Model:                            GLM   Df Residuals:                     1447\n",
      "Model Family:                   Gamma   Df Model:                           12\n",
      "Link Function:                    log   Scale:                        0.019912\n",
      "Method:                          IRLS   Log-Likelihood:                -16853.\n",
      "Date:                Thu, 25 Nov 2021   Deviance:                       32.368\n",
      "Time:                        21:34:12   Pearson chi2:                     28.8\n",
      "No. Iterations:                    16                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================================\n",
      "                                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Intercept                               10.2980      0.047    221.451      0.000      10.207      10.389\n",
      "MSZoning[T.FV]                           0.2056      0.025      8.144      0.000       0.156       0.255\n",
      "MSZoning[T.RH]                           0.3805      0.059      6.467      0.000       0.265       0.496\n",
      "MSZoning[T.RL]                           0.4248      0.045      9.346      0.000       0.336       0.514\n",
      "MSZoning[T.RM]                           0.2979      0.046      6.462      0.000       0.208       0.388\n",
      "Total_Inside_Area                        0.0002   1.02e-05     20.024      0.000       0.000       0.000\n",
      "TotalBsmtSF                              0.0001   1.06e-05     12.849      0.000       0.000       0.000\n",
      "Bathrooms                                0.0626      0.006      9.685      0.000       0.050       0.075\n",
      "GarageCars                               0.0820      0.007     12.533      0.000       0.069       0.095\n",
      "OverallQual                              0.0926      0.004     21.563      0.000       0.084       0.101\n",
      "ExpensiveNeighborhood                    0.0802      0.026      3.064      0.002       0.029       0.132\n",
      "ExpensiveNeighborhood:MSZoning[T.FV]     0.2056      0.025      8.144      0.000       0.156       0.255\n",
      "ExpensiveNeighborhood:MSZoning[T.RH]    -0.2519      0.084     -2.997      0.003      -0.417      -0.087\n",
      "ExpensiveNeighborhood:MSZoning[T.RL]     0.0202      0.027      0.758      0.448      -0.032       0.073\n",
      "ExpensiveNeighborhood:MSZoning[T.RM]     0.1063      0.039      2.755      0.006       0.031       0.182\n",
      "========================================================================================================\n",
      "\n",
      "Log RMSE train: 0.1572\n"
     ]
    }
   ],
   "source": [
    "df_glm = standard_feature_eng(na_clean(train))\n",
    "\n",
    "# add expensive neighborhood feature, just based on top 10 neighborhoods\n",
    "top_10_neighborhoods = ['NoRidge', 'NridgHt', 'StoneBr','Timber','Veenker','Somerst','ClearCr','Crawfor','CollgCr','Blmngtn']\n",
    "df_glm.loc[:, 'ExpensiveNeighborhood'] = df_glm['Neighborhood'].map(lambda x: 1 if x in top_10_neighborhoods else 0)\n",
    "\n",
    "log_link = sm.families.links.log()\n",
    "fit_gamma = glm(\n",
    "    \"SalePrice ~ Total_Inside_Area + TotalBsmtSF + \\\n",
    "    Bathrooms + GarageCars +\\\n",
    "    OverallQual + ExpensiveNeighborhood * MSZoning\", \n",
    "    data = df_glm, \n",
    "    family = sm.families.Gamma(log_link)).fit()\n",
    "print(fit_gamma.summary())\n",
    "\n",
    "yhat_train = fit_gamma.predict(df_glm)\n",
    "resids_train = np.log(yhat_train) - np.log(df_glm.SalePrice)\n",
    "rmse_train = np.mean(resids_train**2)**.5\n",
    "\n",
    "print(\"\\nLog RMSE train: {:.4f}\".format(rmse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Train: 21081.507233659624\n",
      "RMSE Test: 71421.72761145054\n",
      "\n",
      "Log RMSE Train: 0.09416299602547112\n",
      "Log RMSE Test: 0.3737009651878217\n"
     ]
    }
   ],
   "source": [
    "def housing_ensemble_model(df):\n",
    "    '''This is the final model that takes in raw data, and makes predictions'''\n",
    "\n",
    "    # OLS\n",
    "    yhat_ols1 = lr_1.predict(lm_df_clean(df, test_data=True))\n",
    "\n",
    "    # OLS Simple\n",
    "    yhat_ols_simple = lr_overall_quality.predict(lm_overall_quality_df_clean(df, test_data=True))\n",
    "\n",
    "    # GLM\n",
    "    df_glm = standard_feature_eng(na_clean(df), test_data=True)\n",
    "    # add expensive neighborhood feature, just based on top 10 neighborhoods\n",
    "    top_10_neighborhoods = ['NoRidge', 'NridgHt', 'StoneBr','Timber','Veenker','Somerst','ClearCr','Crawfor','CollgCr','Blmngtn']\n",
    "    df_glm.loc[:, 'ExpensiveNeighborhood'] = df_glm['Neighborhood'].map(lambda x: 1 if x in top_10_neighborhoods else 0)\n",
    "    yhat_glm = fit_gamma.predict(df_glm)\n",
    "\n",
    "    # Lasso\n",
    "    lasso_data = lasso_df_clean(df, test_data=True)\n",
    "    yhat_lasso = np.exp(lasso_fit.predict(lasso_data))\n",
    "\n",
    "    # Random forest\n",
    "    yhat_rf = np.exp(rf_reg.predict(rf_df_clean(df, test_data=True)))\n",
    "\n",
    "    # XGB\n",
    "    # xgb_data = xgb.DMatrix(xgb_df_clean(df, test_data=True)) \n",
    "    # yhat_xgb = np.exp(booster.predict(xgb_data))\n",
    "\n",
    "    # make ensemble prediction\n",
    "    #yhat_final = yhat_ols1*(0.025) + yhat_ols_simple * (0.075) + yhat_lasso * (0.5) + yhat_rf * (0.4) # 18920\n",
    "    #yhat_final = yhat_ols1 * .025 + yhat_ols_simple * .075 + yhat_lasso * .5 + yhat_rf * .3 + yhat_glm * .1 # 21474\n",
    "    #yhat_final = yhat_ols1*(0.025) + yhat_ols_simple * (0.075) + yhat_lasso * (0.4) + yhat_rf * (0.3) + yhat_glm * .2 # 22841\n",
    "    #yhat_final = yhat_ols1 * .025 + yhat_ols_simple * .075 + yhat_lasso * .5 + yhat_rf * .3 + yhat_glm * .1 # 21474\n",
    "    yhat_final = yhat_ols1 * .1 + yhat_lasso * .5 + yhat_rf * .3 + yhat_glm * .1 # 21081\n",
    "\n",
    "    return yhat_final\n",
    "\n",
    "\n",
    "# final ensemble model RMSE\n",
    "yhat_train = housing_ensemble_model(train.drop(columns = [\"SalePrice\"]))\n",
    "yhat_test = housing_ensemble_model(test)\n",
    "\n",
    "rmse_train = np.mean((train.SalePrice - yhat_train)**2)**.5\n",
    "rmse_test = np.mean((sample.SalePrice - yhat_test)**2)**.5\n",
    "\n",
    "# evaluate rmse  on the testing data\n",
    "print(\"RMSE Train: {}\".format(rmse_train))\n",
    "print(\"RMSE Test: {}\".format(rmse_test))\n",
    "\n",
    "# log results\n",
    "rmse_train_log = np.mean((np.log(train.SalePrice) - np.log(yhat_train))**2)**.5\n",
    "rmse_test_log = np.mean((np.log(sample.SalePrice) - np.log(yhat_test))**2)**.5\n",
    "# evaluate rmse  on the testing data\n",
    "print(\"\\nLog RMSE Train: {}\".format(rmse_train_log))\n",
    "print(\"Log RMSE Test: {}\".format(rmse_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame({\n",
    "    \"Id\": test.Id,\n",
    "    \"SalePrice\": yhat_test\n",
    "})\n",
    "sample_submission.to_csv(\"~/Desktop/sample_submission_11.25.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 0.13144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9a763963a0c2e3eb3033a1766c128288cb698df5642a2fe22798efe2c630a93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
