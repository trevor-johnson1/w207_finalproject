{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "# data manipulation/viz\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling setups\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm \n",
    "\n",
    "# tree modeling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# other\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# turn off the df['col'] = x assignment warning\n",
    "#pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional directory set-up\n",
    "train = pd.read_csv(\"../../housing_data/train.csv\")\n",
    "test = pd.read_csv(\"../../housing_data/test.csv\")\n",
    "sample = pd.read_csv(\"../../housing_data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data clean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_clean(df):\n",
    "    \n",
    "    # some vars are just too missing so I remove the field\n",
    "    df = df.drop(columns = [\"PoolQC\", \"MiscFeature\"])\n",
    "\n",
    "    # replace some numeric vars w/ median\n",
    "    median_replace_vars = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
    "    for var in median_replace_vars:\n",
    "        df[var].fillna(df[var].median(), inplace = True)\n",
    "    \n",
    "    # replace some num vars w/ 0\n",
    "    zero_replace_vars = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n",
    "    for var in zero_replace_vars:\n",
    "        df[var].fillna(0, inplace = True)\n",
    "    \n",
    "    # replace some cat vars w/ most freq value \n",
    "    df['MasVnrType'].fillna('None', inplace = True)\n",
    "    df['Electrical'].fillna('SBrkr', inplace = True)\n",
    "    df['MSZoning'].fillna('RL', inplace = True)\n",
    "    df['SaleType'].fillna('WD', inplace = True)\n",
    "    df['Utilities'].fillna('AllPub', inplace = True)\n",
    "    df['KitchenQual'].fillna('TA', inplace = True)\n",
    "    df['Functional'].fillna('Typ', inplace = True)\n",
    "\n",
    "    # other cat vars just put missing if there isn't a glaring most popular category\n",
    "    replace_missing_vars = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Exterior1st', \n",
    "        'Exterior2nd', 'FireplaceQu']\n",
    "    for var in replace_missing_vars:\n",
    "        df[var].fillna(\"Missing\", inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function for some standard feature engineering to use in all models\n",
    "def standard_feature_eng(df, test_data = False):\n",
    "    '''Input either the training or test data. \n",
    "    2nd arg set to True if it's the testing data. That way we ignore the final log transformation on sale price'''\n",
    "\n",
    "    # num features to just binarize b/c few houses have the feature\n",
    "    df[\"SwimmingPool\"] = df['PoolArea'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"3SsnPorch\"] = df['3SsnPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"ScreenPorch\"] = df['ScreenPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    # re-factoring vars:\n",
    "    # group the irregularities into 2 factor levels\n",
    "    df['LotShape'] = df['LotShape'].map({'Reg': 'Reg', 'IR1': 'Reg', 'IR2': 'Irreg', 'IR3': 'Irreg'})\n",
    "\n",
    "    # simplifying MSSubClass because we have the year built in another feature\n",
    "    df['MSSubClass'] = df['MSSubClass'].map(lambda x: \n",
    "        \"1_story\"   if (x in (20, 30, 40, 120)) else(\n",
    "        \"1.5_story\" if (x in (45, 50, 150)) else(\n",
    "        \"2_story\"   if (x in (60, 70, 75, 160, 180, 190)) else(\n",
    "        \"split\"     if (x in (80, 85)) else(\n",
    "        \"duplex\"    if (x ==90) else(\n",
    "        \"other\"))))))\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"object\")\n",
    "\n",
    "    # simplifying more vars\n",
    "    # electrical:\n",
    "    df['Electrical'] = df['Electrical'].map(lambda x: \"SBrkr\" if x == \"SBrkr\" else \"Fuse\")\n",
    "    # exterior:\n",
    "    df['Exterior'] = df['Exterior1st'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Expensive\" if (x in (\"VinylSd\", \"CemntBd\", \"Stone\", \"ImStucc\")) else(\n",
    "        \"Cheap\" if (x in (\"BrkComm\", \"AsphShn\", \"CBlock\", \"AsbShng\")) else(\n",
    "        \"Moderate\")))\n",
    "    df = df.drop(columns=['Exterior1st', 'Exterior2nd'])\n",
    "    # garage\n",
    "    df['GarageQual'] = df['GarageQual'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Good\" if (x in (\"Ex\", \"Gd\")) else(\n",
    "        \"Medium\" if (x in (\"TA\")) else(\n",
    "        \"Bad\")))\n",
    "    df['Heating'] = df['Heating'].map(lambda x: \"Gas\" if x in (\"GasA\", \"GasW\") else \"Other\")\n",
    "\n",
    "    # deciding to drop a few features for various reasons\n",
    "    vars_to_drop = [\n",
    "        # not much variation:\n",
    "        \"LowQualFinSF\", \n",
    "        \"LandSlope\", \n",
    "        \"MiscVal\", \n",
    "        \"RoofMatl\",\n",
    "        \"Condition2\",\n",
    "        #\"KitchenAbvGr\" # hardly any variation. But, Deva included in lm's so including it now.\n",
    "        \"PoolArea\", # binarized above\n",
    "        \"Utilities\", # only 1 obs in training data different from regular\n",
    "        \"HouseStyle\" # already explained in other vars\n",
    "        ]\n",
    "    df.drop(columns=vars_to_drop, inplace=True) \n",
    "\n",
    "    # adding a remodeled feature\n",
    "    df['Remodeled'] = (df.YearRemodAdd-df.YearBuilt) == 0\n",
    "\n",
    "    # total inside area will be a sum of 1st and 2nd floor sq ft\n",
    "    df['Total_Inside_Area'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df.drop(columns = ['1stFlrSF', '2ndFlrSF', 'GrLivArea'], inplace = True)\n",
    "\n",
    "    # simplify the bathrooms variable\n",
    "    df['Bathrooms'] = df.BsmtFullBath + 0.5*df.BsmtHalfBath + df.FullBath + 0.5*df.HalfBath\n",
    "    df.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], inplace = True)\n",
    "\n",
    "    # get log of sale price which will be our actual response variable\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df['LogSalePrice'] = np.log(df.SalePrice)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS with 16 features predicting regular sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.850\n",
      "Score on testing data: -17.593\n",
      "Log RMSE on training data: 0.146\n",
      "Log RMSE on testing data: 0.378\n"
     ]
    }
   ],
   "source": [
    "def lm_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    lm_vars = ['LotArea', 'Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'YearRemodAdd', \n",
    "              'BsmtCond', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'YrSold', \n",
    "              'MoSold', 'Remodeled', 'Total_Inside_Area', 'Bathrooms']\n",
    "\n",
    "    df = pd.get_dummies(df[lm_vars], \n",
    "        columns = ['Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'BsmtCond','KitchenQual'], \n",
    "        drop_first=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# data setups\n",
    "X_train = lm_df_clean(train)\n",
    "X_test = lm_df_clean(test, test_data=True)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "# fit to train data\n",
    "lr_1 = LinearRegression(fit_intercept=True).fit(X_train, Y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lr_1.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lr_1.score(X_test ,Y_test)))\n",
    "\n",
    "yhat_train = lr_1.predict(X_train)\n",
    "yhat_test = lr_1.predict(X_test)\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((np.log(yhat_train) - np.log(Y_train))**2))**.5\n",
    "rmse_test = (np.mean((np.log(yhat_test) - np.log(Y_test))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS with 1 variable (overall quality) predicting regular sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.626\n",
      "Score on testing data: -14.882\n",
      "Log RMSE on training data: 0.811\n",
      "Log RMSE on testing data: 1.354\n"
     ]
    }
   ],
   "source": [
    "# data setup function\n",
    "def lm_overall_quality_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "    df = df.loc[:, ['OverallQual']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# data setups\n",
    "X_train = lm_overall_quality_df_clean(train)\n",
    "X_test = lm_overall_quality_df_clean(test, test_data=True)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "# fit to train data\n",
    "lr_overall_quality = LinearRegression(fit_intercept=True).fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lr_overall_quality.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lr_overall_quality.score(X_test ,Y_test)))\n",
    "\n",
    "yhat_train = lr_overall_quality.predict(X_train)\n",
    "yhat_test = lr_overall_quality.predict(X_test)\n",
    "\n",
    "# set negative values to 0.1\n",
    "yhat_train = np.array([0.1 if i < 0 else i for i in yhat_train])\n",
    "yhat_test = [0.1 if i < 0 else i for i in yhat_test]\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((np.log(yhat_train) - np.log(Y_train))**2))**.5\n",
    "rmse_test = (np.mean((np.log(yhat_test) - np.log(Y_test))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso predicting log sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.850\n",
      "Score on testing data: -14.746\n",
      "Log RMSE on training data: 0.155\n",
      "Log RMSE on testing data: 0.357\n"
     ]
    }
   ],
   "source": [
    "# first build one hot encoder based on the training data\n",
    "train_lasso = standard_feature_eng(na_clean(train))\n",
    "enc_lasso = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc_lasso.fit(train_lasso.select_dtypes(include=[\"object\"]))\n",
    "one_hot_columns = pd.get_dummies(train_lasso.select_dtypes(include=[\"object\"])).columns\n",
    "# will use this encoder in the function below\n",
    "\n",
    "# data setup function\n",
    "def lasso_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    # one hot encode using encoder above\n",
    "    categorical_cols = pd.DataFrame(enc_lasso.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_columns\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "\n",
    "    # log transformations\n",
    "    #df[\"GrLivArea\"] = np.log(df[\"GrLivArea\"])\n",
    "    \n",
    "    # select only vars needed\n",
    "    if test_data:\n",
    "        df = df.drop(columns=[\"Id\"])\n",
    "    else:\n",
    "        df = df.drop(columns=[\"Id\"])\n",
    "        df['SalePrice'] = np.log(df['SalePrice'])\n",
    "        #df = df[[\"GrLivArea\",\"OverallQual\", \"SalePrice\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# data setups\n",
    "X_train = lasso_df_clean(train)\n",
    "X_test = lasso_df_clean(test, test_data=True)\n",
    "Y_train = X_train.SalePrice\n",
    "X_train = X_train.drop(columns=['LogSalePrice', 'SalePrice'])\n",
    "Y_test = np.log(sample.SalePrice)\n",
    "\n",
    "# fit to train data\n",
    "lasso_fit = Lasso(alpha=.01).fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "yhat_train = lasso_fit.predict(X_train)\n",
    "yhat_test = lasso_fit.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(lasso_fit.score(X_train,Y_train)))\n",
    "print(\"Score on testing data: {:.3f}\".format(lasso_fit.score(X_test ,Y_test)))\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest to predict log sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.983\n",
      "Score on testing data: -15.389\n",
      "Log RMSE on training data: 0.052\n",
      "Log RMSE on testing data: 0.364\n"
     ]
    }
   ],
   "source": [
    "# first build one hot encoder based on the training data\n",
    "train_rf = standard_feature_eng(na_clean(train))\n",
    "enc_rf = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc_rf.fit(train_rf.select_dtypes(include=[\"object\"]))\n",
    "one_hot_columns = pd.get_dummies(train_rf.select_dtypes(include=[\"object\"])).columns\n",
    "# will use this encoder in the function below\n",
    "\n",
    "# Random forest data clean function\n",
    "def rf_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    # one hot encode using encoder above\n",
    "    categorical_cols = pd.DataFrame(enc_rf.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_columns\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "    \n",
    "    # DO FEATURE ENGINEERING HERE\n",
    "    # drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "df_rf = rf_df_clean(train)\n",
    "df_rf_test = rf_df_clean(test, test_data=True)\n",
    "\n",
    "# run model on best parameters\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators = 900,\n",
    "    max_depth = 25,\n",
    "    max_features = 'auto',\n",
    "    min_samples_split = 2,  \n",
    "    bootstrap = True, \n",
    "    )\n",
    "\n",
    "# fit the model\n",
    "rf_reg = rf_reg.fit(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_rf.LogSalePrice)\n",
    "\n",
    "# Evaluate performance\n",
    "yhat_train = rf_reg.predict(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']))\n",
    "yhat_test = rf_reg.predict(df_rf_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Score on training data: {:.3f}\".format(rf_reg.score(df_rf.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_rf.LogSalePrice)))\n",
    "print(\"Score on testing data: {:.3f}\".format(rf_reg.score(df_rf_test, np.log(sample.SalePrice))))\n",
    "\n",
    "# rmse\n",
    "rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "print(\"Log RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost to predict log sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:6.93301\ttest-rmse:6.99010\n",
      "[1]\ttrain-rmse:4.17002\ttest-rmse:4.23007\n",
      "[2]\ttrain-rmse:2.51226\ttest-rmse:2.57474\n",
      "[3]\ttrain-rmse:1.52339\ttest-rmse:1.59279\n",
      "[4]\ttrain-rmse:0.92837\ttest-rmse:1.01119\n",
      "[5]\ttrain-rmse:0.57398\ttest-rmse:0.68744\n",
      "[6]\ttrain-rmse:0.36331\ttest-rmse:0.51933\n",
      "[7]\ttrain-rmse:0.24082\ttest-rmse:0.44865\n",
      "[8]\ttrain-rmse:0.16872\ttest-rmse:0.41735\n",
      "[9]\ttrain-rmse:0.12559\ttest-rmse:0.40361\n",
      "\n",
      "Log RMSE on training data: 0.126\n",
      "Log RMSE on testing data: 0.404\n"
     ]
    }
   ],
   "source": [
    "# first build one hot encoder based on the training data\n",
    "train_xgb = standard_feature_eng(na_clean(train))\n",
    "enc_xgb = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc_xgb.fit(train_xgb.select_dtypes(include=[\"object\"]))\n",
    "one_hot_columns_xgb = pd.get_dummies(train_xgb.select_dtypes(include=[\"object\"])).columns\n",
    "# will use this encoder in the function below\n",
    "\n",
    "# xgboost data clean function\n",
    "def xgb_df_clean(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = na_clean(df)\n",
    "    df = standard_feature_eng(df, test_data = test_data)\n",
    "\n",
    "    # one hot encode using encoder above\n",
    "    categorical_cols = pd.DataFrame(enc_xgb.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_columns_xgb\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1)\n",
    "    \n",
    "    # DO MORE FEATURE ENGINEERING HERE LATER\n",
    "    df = df.drop(columns = ['Id'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "df_xgb = xgb_df_clean(train)\n",
    "df_xgb_test = xgb_df_clean(test, test_data=True)\n",
    "\n",
    "# get X feature names\n",
    "xgb_cols = np.array(df_xgb.drop(columns = [\"SalePrice\", 'LogSalePrice']).columns)\n",
    "\n",
    "# convert data to DMatrix format\n",
    "dmat_train = xgb.DMatrix(df_xgb.drop(columns = [\"SalePrice\", 'LogSalePrice']), df_xgb['LogSalePrice'], feature_names=xgb_cols)\n",
    "dmat_test = xgb.DMatrix(df_xgb_test, np.log(sample.SalePrice), feature_names=xgb_cols)\n",
    "\n",
    "# train model\n",
    "booster = xgb.train({\n",
    "\n",
    "    \"booster\": \"gbtree\", \n",
    "    \"max_depth\": 30, \n",
    "    \"eta\": .4, \n",
    "    \"gamma\": .01, \n",
    "    \"subsample\": 0.6,\n",
    "    \"lambda\": .7, \n",
    "    \"alpha\": 0, \n",
    "    \"max_bin\": 256, \n",
    "    \"colsample_bytree\": .7, # proportion of features\n",
    "    \"eval_metric\": \"rmse\", \n",
    "    \"objective\": \"reg:squarederror\"\n",
    "    },\n",
    "\n",
    "    dmat_train,\n",
    "    evals=[(dmat_train, \"train\"), (dmat_test, \"test\")] \n",
    ")\n",
    "\n",
    "# Evaluate performance\n",
    "yhat_train = booster.predict(dmat_train)\n",
    "yhat_test = booster.predict(dmat_test)\n",
    "\n",
    "# # rmse\n",
    "rmse_train = (np.mean((yhat_train - np.log(train.SalePrice))**2))**.5\n",
    "rmse_test = (np.mean((yhat_test - np.log(sample.SalePrice))**2))**.5\n",
    "\n",
    "print(\"\\nLog RMSE on training data: {:.3f}\".format(rmse_train))\n",
    "print(\"Log RMSE on testing data: {:.3f}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Train: 28279.128974825577\n",
      "RMSE Test: 65377.90673992918\n",
      "\n",
      "Log RMSE Train: 0.12183455865475633\n",
      "Log RMSE Test: 0.3655684085054176\n"
     ]
    }
   ],
   "source": [
    "def housing_ensemble_model(df):\n",
    "    '''This is the final model that takes in raw data, and makes predictions'''\n",
    "\n",
    "    # OLS\n",
    "    yhat_ols1 = lr_1.predict(lm_df_clean(df, test_data=True))\n",
    "\n",
    "    # OLS Simple\n",
    "    yhat_ols_simple = lr_overall_quality.predict(lm_overall_quality_df_clean(df, test_data=True))\n",
    "\n",
    "    # GLM\n",
    "\n",
    "    # Lasso\n",
    "    lasso_data = lasso_df_clean(df, test_data=True)\n",
    "    yhat_lasso = np.exp(lasso_fit.predict(lasso_data))\n",
    "\n",
    "    # Random forest\n",
    "    yhat_rf = np.exp(rf_reg.predict(rf_df_clean(df, test_data=True)))\n",
    "\n",
    "    # XGB\n",
    "    xgb_data = xgb.DMatrix(xgb_df_clean(df, test_data=True)) \n",
    "    yhat_xgb = np.exp(booster.predict(xgb_data))\n",
    "\n",
    "    # make ensemble prediction\n",
    "    # Do more work to come up w/ weights. This is just a sample\n",
    "    yhat_final = yhat_ols1*(1/5) + yhat_ols_simple * (1/5) + yhat_lasso * (1/5) + yhat_rf * (1/5) + yhat_xgb * (1/5)\n",
    "\n",
    "    return yhat_final\n",
    "\n",
    "\n",
    "# final ensemble model RMSE\n",
    "yhat_train = housing_ensemble_model(train.drop(columns = [\"SalePrice\"]))\n",
    "yhat_test = housing_ensemble_model(test)\n",
    "\n",
    "rmse_train = np.mean((train.SalePrice - yhat_train)**2)**.5\n",
    "rmse_test = np.mean((sample.SalePrice - yhat_test)**2)**.5\n",
    "\n",
    "# evaluate rmse  on the testing data\n",
    "print(\"RMSE Train: {}\".format(rmse_train))\n",
    "print(\"RMSE Test: {}\".format(rmse_test))\n",
    "\n",
    "# log results\n",
    "rmse_train_log = np.mean((np.log(train.SalePrice) - np.log(yhat_train))**2)**.5\n",
    "rmse_test_log = np.mean((np.log(sample.SalePrice) - np.log(yhat_test))**2)**.5\n",
    "# evaluate rmse  on the testing data\n",
    "print(\"\\nLog RMSE Train: {}\".format(rmse_train_log))\n",
    "print(\"Log RMSE Test: {}\".format(rmse_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame({\n",
    "    \"Id\": test.Id,\n",
    "    \"SalePrice\": yhat_test\n",
    "})\n",
    "sample_submission.to_csv(\"~/Desktop/sample_submission_11-13-2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 0.14089\n",
    "\n",
    "Rank: 2042 / 4683 = 43.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9a763963a0c2e3eb3033a1766c128288cb698df5642a2fe22798efe2c630a93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
