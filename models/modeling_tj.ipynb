{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "\n",
    "## Deva Kulkarni, Jared Dec, Marc Semonick, Trevor Johnson\n",
    "\n",
    "## October 2021\n",
    "\n",
    "<br>\n",
    "\n",
    "Competition Link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Goal: Predict sales price for each house (`SalePrice`). RMSE on log(pred) - log(actual) will be the evaluation metric. \n",
    "\n",
    "Inference Problem:  Given known variables about a house, accurately predict its sale price.\n",
    "\n",
    "Deliverable: Final submission dataset should contain only the two fields `ID` and `SalePrice`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import math\n",
    "import os \n",
    "\n",
    "# data manipulation/viz\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# tree modeling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional directory set-up\n",
    "working_dir = !pwd\n",
    "train = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"test.csv\"))\n",
    "sample = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1460, 81)\n",
      "test shape: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape: {}\".format(train.shape))\n",
    "print(\"test shape: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count number of missing values per variable:\n",
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GarageYrBlt       81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "\n",
      "Number of missing values after running na_clean()\n",
      "Missing values in train: 0\n",
      "Missing values in test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Count number of missing values per variable:\")\n",
    "print(train.isnull().sum()[train.isnull().sum() != 0])\n",
    "\n",
    "# function to clean the missing values\n",
    "def na_clean(df):\n",
    "    \n",
    "    # some vars are just too missing so I remove the field\n",
    "    df = df.drop(columns = [\"PoolQC\", \"MiscFeature\"])\n",
    "\n",
    "    # replace some numeric vars w/ median\n",
    "    median_replace_vars = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
    "    for var in median_replace_vars:\n",
    "        df[var].fillna(df[var].median(), inplace = True)\n",
    "    \n",
    "    # replace some num vars w/ 0\n",
    "    zero_replace_vars = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n",
    "    for var in zero_replace_vars:\n",
    "        df[var].fillna(0, inplace = True)\n",
    "    \n",
    "    # replace some cat vars w/ most freq value \n",
    "    df['MasVnrType'].fillna('None', inplace = True)\n",
    "    df['Electrical'].fillna('SBrkr', inplace = True)\n",
    "    df['MSZoning'].fillna('RL', inplace = True)\n",
    "    df['SaleType'].fillna('WD', inplace = True)\n",
    "    df['Utilities'].fillna('AllPub', inplace = True)\n",
    "    df['KitchenQual'].fillna('TA', inplace = True)\n",
    "    df['Functional'].fillna('Typ', inplace = True)\n",
    "\n",
    "    # other cat vars just put missing if there isn't a glaring most popular category\n",
    "    replace_missing_vars = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Exterior1st', \n",
    "        'Exterior2nd', 'FireplaceQu']\n",
    "    for var in replace_missing_vars:\n",
    "        df[var].fillna(\"Missing\", inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = na_clean(train)\n",
    "test = na_clean(test)\n",
    "\n",
    "# make sure there are no more missing values\n",
    "print(\"\\nNumber of missing values after running na_clean()\")\n",
    "print(\"Missing values in train: {}\".format(train.isnull().sum().sum()))\n",
    "print(\"Missing values in test: {}\".format(test.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Simplifying the data for any future model we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep adapting this formula to change how we feature engineer\n",
    "def feature_eng(df, test_data = False):\n",
    "    '''Input either the training or test data. \n",
    "    2nd arg set to True if it's the testing data. That way we ignore the final log transformation on sale price'''\n",
    "\n",
    "    # num features to just binarize b/c few houses have the feature\n",
    "    df[\"SwimmingPool\"] = df['PoolArea'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"3SsnPorch\"] = df['3SsnPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"ScreenPorch\"] = df['ScreenPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    # re-factoring vars:\n",
    "    # group the irregularities into 2 factor levels\n",
    "    df['LotShape'] = df['LotShape'].map({'Reg': 'Reg', 'IR1': 'Reg', 'IR2': 'Irreg', 'IR3': 'Irreg'})\n",
    "\n",
    "    # simplifying MSSubClass because we have the year built in another feature\n",
    "    df['MSSubClass'] = df['MSSubClass'].map(lambda x: \n",
    "        \"1_story\"   if (x in (20, 30, 40, 120)) else(\n",
    "        \"1.5_story\" if (x in (45, 50, 150)) else(\n",
    "        \"2_story\"   if (x in (60, 70, 75, 160, 180, 190)) else(\n",
    "        \"split\"     if (x in (80, 85)) else(\n",
    "        \"duplex\"    if (x ==90) else(\n",
    "        \"other\"))))))\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"object\")\n",
    "\n",
    "    # deciding to drop a few features for various reasons\n",
    "    vars_to_drop = [\n",
    "        \"LowQualFinSF\", # hardly any variation\n",
    "        \"LandSlope\", # not much variation\n",
    "        \"PoolArea\", # binarized above\n",
    "        \"MiscVal\", # not much variation\n",
    "        \"Utilities\", # only 1 obs in training data different from regular\n",
    "        #\"KitchenAbvGr\" # hardly any variation. But, Deva included in lm's so including it now.\n",
    "        ]\n",
    "    df.drop(columns=vars_to_drop, inplace=True) \n",
    "\n",
    "    # adding a remodeled feature\n",
    "    df['Remodeled'] = (df.YearRemodAdd-df.YearBuilt) == 0\n",
    "\n",
    "    # total inside area will be a sum of 1st and 2nd floor sq ft\n",
    "    df['Total_Inside_Area'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df.drop(columns = ['1stFlrSF', '2ndFlrSF', 'GrLivArea'], inplace = True)\n",
    "\n",
    "    # simplify the bathrooms variable\n",
    "    df['Bathrooms'] = df.BsmtFullBath + 0.5*df.BsmtHalfBath + df.FullBath + 0.5*df.HalfBath\n",
    "    df.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], inplace = True)\n",
    "\n",
    "    # get log of sale price which will be our actual response variable\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df['LogSalePrice'] = np.log(df.SalePrice)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function on test and train\n",
    "train = feature_eng(train)\n",
    "test = feature_eng(test, test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prep data for a very baseline linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_df_clean(df):\n",
    "\n",
    "    lm_vars = ['LotArea', 'Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'YearRemodAdd', \n",
    "              'BsmtCond', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'YrSold', \n",
    "              'MoSold', 'Remodeled', 'Total_Inside_Area', 'Bathrooms']\n",
    "\n",
    "    df = pd.get_dummies(df[lm_vars], \n",
    "        columns = ['Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'BsmtCond','KitchenQual'], \n",
    "        drop_first=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train/test data all ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = lm_df_clean(train)\n",
    "X_test = lm_df_clean(test)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline linear model is very overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit on training data: 0.850\n",
      "Fit on testing data: -17.593\n"
     ]
    }
   ],
   "source": [
    "# Decent R2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_1 = LinearRegression(fit_intercept=True)\n",
    "lr_1.fit(X_train,Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(lr_1.score(X_train,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(lr_1.score(X_test ,Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same feature set with a baseline RandomForest is also overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit on training data: 1.000\n",
      "Fit on testing data: -18.977\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state = 0)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(dt.score(X_train,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(dt.score(X_test ,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic linear model with single coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit on training data: 0.626\n",
      "Fit on testing data: -14.882\n"
     ]
    }
   ],
   "source": [
    "#Simplest possible model - Chose variable with highest coefficient from initial correlation matrix\n",
    "\n",
    "X_train_2 = train.drop(train.columns.difference(['OverallQual']), 1)\n",
    "X_test_2 = test.drop(test.columns.difference(['OverallQual']), 1)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "lr_2 = LinearRegression(fit_intercept=True)\n",
    "lr_2.fit(X_train_2,Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(lr_2.score(X_train_2,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(lr_2.score(X_test_2 ,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data\n",
    "train_dummies = pd.get_dummies(train)\n",
    "\n",
    "# create the train and test splits\n",
    "dev_train, dev_test = train_test_split(train_dummies, test_size=0.3, random_state=1)\n",
    "\n",
    "# initialize model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# specify hyperparameters to tune\n",
    "# first use RandomizedSearchCV to narrow down list\n",
    "grid_random = {\n",
    "    \"n_estimators\": np.arange(100, 601, 200), \n",
    "    'max_features': ['auto', 'sqrt'], \n",
    "    'max_depth': np.arange(1, 11, 3), \n",
    "    'min_samples_split': [10, 20, 30],\n",
    "}\n",
    "\n",
    "# now randomly try out these hyperparameters to find the best\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, \n",
    "    param_distributions = grid_random,\n",
    "    n_iter = 50, \n",
    "    cv = 3,\n",
    "    random_state = 1,\n",
    "    n_jobs = -1 \n",
    ")\n",
    "\n",
    "# random search through a few parameter options\n",
    "results_random = rf_random.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "# check out the best parameters\n",
    "print(results_random.best_params_)\n",
    "# {'n_estimators': 500, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 40, 'max_features': 'auto', 'min_samples_split': 5, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# now run grid search on a smaller range of values\n",
    "grid = {\n",
    "    \"n_estimators\": [500, 600],\n",
    "    'max_depth': [10, 20, 40],\n",
    "    'min_samples_split': [5, 10],\n",
    "}\n",
    "\n",
    "# specify model\n",
    "rf = RandomForestRegressor(bootstrap=True, max_features='auto')\n",
    "\n",
    "# put our parameters into GridSearch\n",
    "gscv = GridSearchCV(\n",
    "    estimator = rf, \n",
    "    param_grid = grid, \n",
    "    n_jobs = -1, \n",
    "    cv = 3)\n",
    "\n",
    "# run the search:\n",
    "results = gscv.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "# check out the best parameters\n",
    "print(results.best_params_)\n",
    "# {'max_depth': 40, 'max_features': 'auto', 'min_samples_split': 5, 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.101\n",
      "RMSE: 0.1545\n"
     ]
    }
   ],
   "source": [
    "# FINALLY, we know the best parameters to use. So let's train the model\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators = 500,\n",
    "    max_depth = 40, \n",
    "    max_features = 'auto',\n",
    "    min_samples_split = 5,  \n",
    "    bootstrap = True, \n",
    "    )\n",
    "\n",
    "# fit the model\n",
    "rf_reg.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "# evaluate it's performance on the unforseen testing set\n",
    "yhat = rf_reg.predict(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']))\n",
    "resids = yhat - ds1_test.ya\n",
    "\n",
    "# evaluate rmse and mae on the testing data\n",
    "print(\"MAE: \" + str(round(mean(resids.apply(lambda x: math.fabs(x))),4)))\n",
    "print(\"RMSE: \" + str(round(mean(resids**2)**.5,4)))\n",
    "# MAE: 0.0999\n",
    "# RMSE: 0.154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on train: 0.975\n",
      "Score on train: 0.874\n"
     ]
    }
   ],
   "source": [
    "# score on train\n",
    "rf_train_score = rf_reg.score(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "print(\"Score on train: {:.3f}\".format(rf_train_score))\n",
    "\n",
    "# score on test/dev\n",
    "rf_dev_score = rf_reg.score(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_test.LogSalePrice)\n",
    "print(\"Score on train: {:.3f}\".format(rf_dev_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9a763963a0c2e3eb3033a1766c128288cb698df5642a2fe22798efe2c630a93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
