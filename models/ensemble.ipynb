{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "This notebook combines the final models we have so far, without any scratch work fluff. This uses models that are split between high cost neighborhoods, and low cost neighborhoods. Refine how we want to split up the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os \n",
    "import re\n",
    "\n",
    "# data manipulation/viz\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling setups\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm \n",
    "\n",
    "# tree modeling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# other\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# turn off the df['col'] = x assignment warning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv(\"../../housing_data/train.csv\")\n",
    "test = pd.read_csv(\"../../housing_data/test.csv\")\n",
    "sample = pd.read_csv(\"../../housing_data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na cleaning\n",
    "def na_clean(df):\n",
    "    \n",
    "    # some vars are just too missing so I remove the field\n",
    "    df = df.drop(columns = [\"PoolQC\", \"MiscFeature\"])\n",
    "\n",
    "    # replace some numeric vars w/ median\n",
    "    median_replace_vars = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
    "    for var in median_replace_vars:\n",
    "        df.loc[:, var].fillna(df[var].median(), inplace = True)\n",
    "    \n",
    "    # replace some num vars w/ 0\n",
    "    zero_replace_vars = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n",
    "    for var in zero_replace_vars:\n",
    "        df.loc[:, var].fillna(0, inplace = True)\n",
    "    \n",
    "    # replace some cat vars w/ most freq value \n",
    "    df.loc[:, 'MasVnrType'].fillna('None', inplace = True)\n",
    "    df.loc[:, 'Electrical'].fillna('SBrkr', inplace = True)\n",
    "    df.loc[:, 'MSZoning'].fillna('RL', inplace = True)\n",
    "    df.loc[:, 'SaleType'].fillna('WD', inplace = True)\n",
    "    df.loc[:, 'Utilities'].fillna('AllPub', inplace = True)\n",
    "    df.loc[:, 'KitchenQual'].fillna('TA', inplace = True)\n",
    "    df.loc[:, 'Functional'].fillna('Typ', inplace = True)\n",
    "\n",
    "    # other cat vars just put \"missing\" if there isn't an obvious most popular category\n",
    "    replace_missing_vars = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Exterior1st', \n",
    "        'Exterior2nd', 'FireplaceQu']\n",
    "    for var in replace_missing_vars:\n",
    "        df.loc[:, var].fillna(\"Missing\", inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for some standard feature engineering to use in all models\n",
    "def standard_feature_eng(df, test_data = False):\n",
    "    '''Input either the training or test data. \n",
    "    2nd arg set to True if it's the testing data. That way we ignore the final log transformation on sale price'''\n",
    "\n",
    "\n",
    "    #-----OUTLIERS-----#\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df = df.loc[(df['LotFrontage'] < 250) & (df['SalePrice'] < 700000) & (df['LotArea'] < 100000)]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-----SIMPLIFYING VARS-----#\n",
    "\n",
    "    # num features to just binarize b/c few houses have the feature\n",
    "    df.loc[:, \"SwimmingPool\"] = df['PoolArea'].map(lambda x: 0 if x==0 else 1)\n",
    "    \n",
    "    # combining porch features into 1 later on instead\n",
    "    #df[\"3SsnPorch\"] = df['3SsnPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "    #df[\"ScreenPorch\"] = df['ScreenPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    # re-factoring vars:\n",
    "    # group the irregularities into 2 factor levels\n",
    "    df.loc[:, 'LotShape'] = df['LotShape'].map({'Reg': 'Reg', 'IR1': 'Reg', 'IR2': 'Irreg', 'IR3': 'Irreg'})\n",
    "\n",
    "    # simplifying MSSubClass because we have the year built in another feature\n",
    "    df.loc[:, 'MSSubClass'] = df['MSSubClass'].map(lambda x: \n",
    "        \"1_story\"   if (x in (20, 30, 40, 120)) else(\n",
    "        \"1.5_story\" if (x in (45, 50, 150)) else(\n",
    "        \"2_story\"   if (x in (60, 70, 75, 160, 180, 190)) else(\n",
    "        \"split\"     if (x in (80, 85)) else(\n",
    "        \"duplex\"    if (x ==90) else(\n",
    "        \"other\"))))))\n",
    "    df.loc[:, 'MSSubClass'] = df['MSSubClass'].astype(\"object\")\n",
    "\n",
    "    # electrical:\n",
    "    df.loc[:, 'Electrical'] = df['Electrical'].map(lambda x: \"SBrkr\" if x == \"SBrkr\" else \"Fuse\")\n",
    "    \n",
    "    # exterior:\n",
    "    df.loc[:, 'Exterior'] = df['Exterior1st'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Expensive\" if (x in (\"VinylSd\", \"CemntBd\", \"Stone\", \"ImStucc\")) else(\n",
    "        \"Cheap\" if (x in (\"BrkComm\", \"AsphShn\", \"CBlock\", \"AsbShng\")) else(\n",
    "        \"Moderate\")))\n",
    "    df = df.drop(columns=['Exterior1st', 'Exterior2nd'])\n",
    "    \n",
    "    # garage\n",
    "    df.loc[:, 'GarageQual'] = df['GarageQual'].map(lambda x: \n",
    "        # group exterior into simplified var based on average prices\n",
    "        \"Good\" if (x in (\"Ex\", \"Gd\")) else(\n",
    "        \"Medium\" if (x in (\"TA\")) else(\n",
    "        \"Bad\")))\n",
    "    df.loc[:, 'Heating'] = df['Heating'].map(lambda x: \"Gas\" if x in (\"GasA\", \"GasW\") else \"Other\")\n",
    "\n",
    "    # total inside area will be a sum of 1st and 2nd floor sq ft\n",
    "    df.loc[:, 'Total_Inside_Area'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df.drop(columns = ['1stFlrSF', '2ndFlrSF', 'GrLivArea'], inplace = True)\n",
    "\n",
    "    # add porch square feet together\n",
    "    df.loc[:, 'PorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n",
    "    df.drop(columns = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'], inplace=True)\n",
    "\n",
    "    # simplify the bathrooms variable\n",
    "    df.loc[:, 'Bathrooms'] = df.BsmtFullBath + 0.5*df.BsmtHalfBath + df.FullBath + 0.5*df.HalfBath\n",
    "    df.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-----DROPPING VARIABLES-----#\n",
    "\n",
    "    # deciding to drop a few features for various reasons\n",
    "    vars_to_drop = [\n",
    "        # not much variation:\n",
    "        \"LowQualFinSF\", \n",
    "        \"LandSlope\", \n",
    "        \"MiscVal\", \n",
    "        \"RoofMatl\",\n",
    "        #\"Condition2\",\n",
    "        #\"KitchenAbvGr\" \n",
    "        \"PoolArea\", # binarized above\n",
    "        \"Utilities\", # only 1 obs in training data different from regular\n",
    "        \"HouseStyle\" # already explained in other vars\n",
    "        ]\n",
    "    df.drop(columns=vars_to_drop, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-----ADDING NEW FEATURES-----#\n",
    "\n",
    "    # remodeled feature\n",
    "    df.loc[:, 'Remodeled'] = np.array((df.YearRemodAdd-df.YearBuilt) == 0, dtype=int)\n",
    "\n",
    "    # expensive neighborhoods\n",
    "    top_10_neighborhoods = ['NoRidge', 'NridgHt', 'StoneBr','Timber','Veenker','Somerst','ClearCr','Crawfor','CollgCr','Blmngtn']\n",
    "    df.loc[:, 'ExpensiveNeighborhood'] = df['Neighborhood'].map(lambda x: 1 if x in top_10_neighborhoods else 0)\n",
    "\n",
    "    # get log of sale price which will be our actual response variable\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df.loc[:, 'LogSalePrice'] = np.log(df.SalePrice)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more streamlined pre-processing for other functions\n",
    "def encode_and_standardize(df, test_data=False):\n",
    "    \n",
    "    # run normal data clean steps above\n",
    "    df = standard_feature_eng(na_clean(df), test_data=test_data)\n",
    "\n",
    "    # if it's train data, we don't want to standardize the response variable\n",
    "    if not test_data:\n",
    "        df_y = df[['SalePrice', 'LogSalePrice']]\n",
    "        df = df.drop(columns=['SalePrice', 'LogSalePrice'])\n",
    "        \n",
    "    # build one hot encoder based on the regular training data\n",
    "    train_one_hot = standard_feature_eng(na_clean(train)).drop(columns=['SalePrice', 'LogSalePrice'])\n",
    "    enc = OneHotEncoder(handle_unknown = 'ignore').fit(train_one_hot.select_dtypes(include=[\"object\"]))\n",
    "    one_hot_column_nms = pd.get_dummies(train_one_hot.select_dtypes(include=[\"object\"])).columns\n",
    "\n",
    "    # one hot encode input df\n",
    "    categorical_cols = pd.DataFrame(enc.transform(df.select_dtypes(include=[\"object\"])).toarray())\n",
    "    categorical_cols.columns = one_hot_column_nms\n",
    "    categorical_cols = categorical_cols.astype(\"int\")\n",
    "    \n",
    "    # align the indexes before the merge\n",
    "    df.index = categorical_cols.index\n",
    "    df = pd.concat([categorical_cols, df.select_dtypes(exclude=[\"object\"])], axis=1) \n",
    "        \n",
    "    # need col names later\n",
    "    all_col_names = df.columns\n",
    "    \n",
    "    # standardize the data between [0, 1]\n",
    "    myMinMax = MinMaxScaler()\n",
    "    df = pd.DataFrame(myMinMax.fit_transform(df))\n",
    "    df.columns = all_col_names\n",
    "\n",
    "    # if it's train data, bring back in the response variable\n",
    "    if not test_data:\n",
    "        df_y.index = df.index\n",
    "        df = pd.concat([df, df_y], axis=1) \n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "# works\n",
    "#encode_and_standardize(test, test_data=True)\n",
    "#encode_and_standardize(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same random forest data cleaner\n",
    "def rf_df_clean_high_low(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = encode_and_standardize(df, test_data=test_data)\n",
    "    \n",
    "    # drop neighborhood var\n",
    "    df = df.drop(columns=list(filter(lambda x: re.search(\"^Neighborhood_\", x), df.columns)))\n",
    "    \n",
    "    # DO FEATURE ENGINEERING HERE\n",
    "\n",
    "    # drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into high and low value neighborhoods\n",
    "train_high = rf_df_clean_high_low(train)\n",
    "train_high = train_high.loc[train_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "train_low = rf_df_clean_high_low(train)\n",
    "train_low = train_low.loc[train_low.ExpensiveNeighborhood == 0]\n",
    "\n",
    "# split up the test data too just like i will in the model\n",
    "test_high = rf_df_clean_high_low(test, test_data=True)\n",
    "test_high_y = np.log(sample.loc[test_high.ExpensiveNeighborhood == 1]['SalePrice'])\n",
    "test_high = test_high.loc[test_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "test_low = rf_df_clean_high_low(test, test_data=True)\n",
    "test_low_y = np.log(sample.loc[test_low.ExpensiveNeighborhood == 0]['SalePrice'])\n",
    "test_low = test_low.loc[test_low.ExpensiveNeighborhood == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing to manually introduce a larger penalty\n",
    "final_alpha = .01\n",
    "\n",
    "# fit model high\n",
    "lasso_high = Lasso(alpha=final_alpha)\\\n",
    "    .fit(train_high.drop(columns=['SalePrice', 'LogSalePrice']), train_high.LogSalePrice)\n",
    "\n",
    "# fit model low\n",
    "lasso_low = Lasso(alpha=final_alpha)\\\n",
    "    .fit(train_low.drop(columns=['SalePrice', 'LogSalePrice']), train_low.LogSalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lasso combination function\n",
    "def lasso_high_low(df):\n",
    "\n",
    "    # create id to put df back into same exact order\n",
    "    df['Id'] = list(range(0, df.shape[0]))\n",
    "\n",
    "    # split dataset into high low\n",
    "    df_high = df.loc[df.ExpensiveNeighborhood == 1]\n",
    "    df_low = df.loc[df.ExpensiveNeighborhood == 0]\n",
    "\n",
    "    # run lasso on high/low\n",
    "    df_high = df_high.assign(yhat = lasso_high.predict(df_high.drop(columns=\"Id\")))\n",
    "    df_low = df_low.assign(yhat = lasso_low.predict(df_low.drop(columns=\"Id\")))\n",
    "\n",
    "    # combine, sort values, and return results\n",
    "    df_preds = pd.concat([df_high[['Id', 'yhat']], df_low[['Id', 'yhat']]], axis=0)\n",
    "    df_preds = df_preds.sort_values(\"Id\", ascending=True)\n",
    "    \n",
    "    return np.array(df_preds.yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "yhat_lasso = lasso_high_low(rf_df_clean_high_low(test, test_data=True))\n",
    "resids = yhat_lasso - np.log(sample.SalePrice)\n",
    "rmse = np.mean(resids**2)**.5\n",
    "print(\"Lasso Log RMSE Splitting up homes: {:.4f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest data clean function\n",
    "def rf_df_clean_high_low(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = encode_and_standardize(df, test_data=test_data)\n",
    "    \n",
    "    # drop neighborhood var\n",
    "    df = df.drop(columns=list(filter(lambda x: re.search(\"^Neighborhood_\", x), df.columns)))\n",
    "    \n",
    "    # DO FEATURE ENGINEERING HERE\n",
    "\n",
    "    # drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into high and low value neighborhoods\n",
    "train_high = rf_df_clean_high_low(train)\n",
    "train_high = train_high.loc[train_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "train_low = rf_df_clean_high_low(train)\n",
    "train_low = train_low.loc[train_low.ExpensiveNeighborhood == 0]\n",
    "\n",
    "# split up the test data too just like i will in the model\n",
    "test_high = rf_df_clean_high_low(test, test_data=True)\n",
    "test_high_y = np.log(sample.loc[test_high.ExpensiveNeighborhood == 1]['SalePrice'])\n",
    "test_high = test_high.loc[test_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "test_low = rf_df_clean_high_low(test, test_data=True)\n",
    "test_low_y = np.log(sample.loc[test_low.ExpensiveNeighborhood == 0]['SalePrice'])\n",
    "test_low = test_low.loc[test_low.ExpensiveNeighborhood == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high cost RF\n",
    "rf_reg_high = RandomForestRegressor(\n",
    "    n_estimators = 400,\n",
    "    max_depth = 15, #17\n",
    "    max_features = .33, # .33\n",
    "    min_samples_split = 2,\n",
    "    ccp_alpha = 1e-3, #0.001\n",
    "    #bootstrap = True, \n",
    "    random_state=1\n",
    "    )\n",
    "\n",
    "# fit the model\n",
    "rf_reg_high.fit(train_high.drop(columns = [\"SalePrice\", 'LogSalePrice']), train_high.LogSalePrice)\n",
    "\n",
    "\n",
    "# low cost rf\n",
    "rf_reg_low = RandomForestRegressor(\n",
    "    n_estimators = 450, # 500\n",
    "    max_depth = 14, # 22\n",
    "    max_features = .33, # 'auto',\n",
    "    min_samples_split=3, \n",
    "    ccp_alpha=1e-6, \n",
    "    bootstrap = True, \n",
    "    random_state=1\n",
    "    ) # 0.3533\n",
    "\n",
    "# fit the model\n",
    "rf_reg_low.fit(train_low.drop(columns = [\"SalePrice\", 'LogSalePrice']), train_low.LogSalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to combine high and low\n",
    "def rf_high_low(df):\n",
    "\n",
    "    # create id to put df back into same exact order\n",
    "    df['Id'] = list(range(0, df.shape[0]))\n",
    "\n",
    "    # split dataset into high low\n",
    "    df_high = df.loc[df.ExpensiveNeighborhood == 1]\n",
    "    df_low = df.loc[df.ExpensiveNeighborhood == 0]\n",
    "\n",
    "    # run rf on high/low\n",
    "    df_high = df_high.assign(yhat = rf_reg_high.predict(df_high.drop(columns=\"Id\")))\n",
    "    df_low = df_low.assign(yhat = rf_reg_low.predict(df_low.drop(columns=\"Id\")))\n",
    "\n",
    "    # combine, sort values, and return results\n",
    "    df_preds = pd.concat([df_high[['Id', 'yhat']], df_low[['Id', 'yhat']]], axis=0)\n",
    "    df_preds = df_preds.sort_values(\"Id\", ascending=True)\n",
    "    return np.array(df_preds.yhat)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "yhat_rf = rf_high_low(rf_df_clean_high_low(test, test_data=True))\n",
    "resids = yhat_rf - np.log(sample.SalePrice)\n",
    "rmse_rf = np.mean(resids**2)**.5\n",
    "print(\"RF Log RMSE Splitting up homes: {:.4f}\".format(rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest data clean function\n",
    "def rf_df_clean_high_low(df, test_data = False):\n",
    "\n",
    "    # first run standard data cleaning steps\n",
    "    df = encode_and_standardize(df, test_data=test_data)\n",
    "    \n",
    "    # drop neighborhood var\n",
    "    df = df.drop(columns=list(filter(lambda x: re.search(\"^Neighborhood_\", x), df.columns)))\n",
    "    \n",
    "    # DO FEATURE ENGINEERING HERE\n",
    "\n",
    "    # drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into high and low value neighborhoods\n",
    "train_high = rf_df_clean_high_low(train)\n",
    "train_high = train_high.loc[train_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "train_low = rf_df_clean_high_low(train)\n",
    "train_low = train_low.loc[train_low.ExpensiveNeighborhood == 0]\n",
    "\n",
    "# split up the test data too just like i will in the model\n",
    "test_high = rf_df_clean_high_low(test, test_data=True)\n",
    "test_high_y = np.log(sample.loc[test_high.ExpensiveNeighborhood == 1]['SalePrice'])\n",
    "test_high = test_high.loc[test_high.ExpensiveNeighborhood == 1]\n",
    "\n",
    "test_low = rf_df_clean_high_low(test, test_data=True)\n",
    "test_low_y = np.log(sample.loc[test_low.ExpensiveNeighborhood == 0]['SalePrice'])\n",
    "test_low = test_low.loc[test_low.ExpensiveNeighborhood == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high cost xgb\n",
    "xgb_high = xgb.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    verbosity=0,\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    learning_rate=.05, \n",
    "    \n",
    "    reg_alpha=0, \n",
    "    reg_lambda=.1, \n",
    "\n",
    "    subsample=.5, \n",
    "    colsample_bytree=.7, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# fit to train\n",
    "xgb_high.fit(train_high.drop(columns=[\"SalePrice\", \"LogSalePrice\"]), train_high['LogSalePrice'])\n",
    "\n",
    "# low cost xgb\n",
    "xgb_low = xgb.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    verbosity=0,\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    learning_rate=.05, \n",
    "    \n",
    "    reg_alpha=0, \n",
    "    reg_lambda=.1, \n",
    "\n",
    "    subsample=.5, \n",
    "    colsample_bytree=.7, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# fit to train\n",
    "xgb_low.fit(train_low.drop(columns=[\"SalePrice\", \"LogSalePrice\"]), train_low['LogSalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to combine high and low\n",
    "def xgb_high_low(df):\n",
    "\n",
    "    # create id to put df back into same exact order\n",
    "    df['Id'] = list(range(0, df.shape[0]))\n",
    "\n",
    "    # split dataset into high low\n",
    "    df_high = df.loc[df.ExpensiveNeighborhood == 1]\n",
    "    df_low = df.loc[df.ExpensiveNeighborhood == 0]\n",
    "\n",
    "    # run rf on high/low\n",
    "    df_high = df_high.assign(yhat = xgb_high.predict(df_high.drop(columns=\"Id\")))\n",
    "    df_low = df_low.assign(yhat = xgb_low.predict(df_low.drop(columns=\"Id\")))\n",
    "\n",
    "    # combine, sort values, and return results\n",
    "    df_preds = pd.concat([df_high[['Id', 'yhat']], df_low[['Id', 'yhat']]], axis=0)\n",
    "    df_preds = df_preds.sort_values(\"Id\", ascending=True)\n",
    "    \n",
    "    return np.array(df_preds.yhat)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "yhat_xgb = xgb_high_low(rf_df_clean_high_low(test, test_data=True))\n",
    "resids = yhat_xgb - np.log(sample.SalePrice)\n",
    "rmse = np.mean(resids**2)**.5\n",
    "print(\"XGB Log RMSE Splitting up homes: {:.4f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in brand new data to make sure it all works\n",
    "train = pd.read_csv(\"../../housing_data/train.csv\")\n",
    "test = pd.read_csv(\"../../housing_data/test.csv\")\n",
    "sample = pd.read_csv(\"../../housing_data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final ensemble model\n",
    "def housing_ensemble_model(df, test_data=True, output_type=\"preds\"):\n",
    "    '''This is the final model that takes in raw data, and makes predictions'''\n",
    "\n",
    "\n",
    "    # Lasso\n",
    "    yhat_lasso = lasso_high_low(rf_df_clean_high_low(df, test_data=test_data))\n",
    "\n",
    "    # Random forest\n",
    "    yhat_rf = rf_high_low(rf_df_clean_high_low(df, test_data=test_data))\n",
    "\n",
    "    # XGB\n",
    "    yhat_xgb = xgb_high_low(rf_df_clean_high_low(df, test_data=test_data))\n",
    "\n",
    "    # make ensemble prediction\n",
    "    # Do more work to come up w/ weights. This is just a sample\n",
    "    yhat_final = (yhat_lasso + yhat_rf + yhat_xgb) / 3\n",
    "\n",
    "    if output_type == \"preds\":\n",
    "\n",
    "        return yhat_final\n",
    "\n",
    "    elif output_type == \"df\":\n",
    "        result_df = pd.DataFrame({\n",
    "            \"lasso\": yhat_lasso, \n",
    "            \"rf\": yhat_rf, \n",
    "            \"xgb\": yhat_xgb, \n",
    "            \"ensemble\": yhat_final\n",
    "        })\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    elif output_type == \"test performances\":\n",
    "        result_df = {\n",
    "            \"lasso\": np.mean((np.log(sample.SalePrice) - yhat_lasso)**2)**.5,\n",
    "            \"rf\": np.mean((np.log(sample.SalePrice) - yhat_rf)**2)**.5,\n",
    "            \"xgb\": np.mean((np.log(sample.SalePrice) - yhat_xgb)**2)**.5,\n",
    "            \"ensemble\": np.mean((np.log(sample.SalePrice) - yhat_final)**2)**.5, \n",
    "        }\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    else:\n",
    "        return yhat_final\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "housing_ensemble_model(test)\n",
    "housing_ensemble_model(train, test_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see accuracies\n",
    "housing_ensemble_model(test, test_data=True, output_type=\"test performances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see distribution of the accuracies\n",
    "result_df = housing_ensemble_model(test, output_type=\"df\")\n",
    "result_df = pd.concat([result_df, \n",
    "    pd.DataFrame({\"Actual\": np.log(sample.SalePrice)})\n",
    "    ], axis=1)\n",
    "\n",
    "result_df = result_df.melt(var_name=\"Type\", value_name=\"log_sale_price\")\n",
    "result_df\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.displot(result_df, x=\"log_sale_price\", hue=\"Type\", kind=\"kde\", aspect=2).set_axis_labels(x_var=\"Log Sale Price\")\n",
    "plt.title(\"Model Distributions\",fontdict={\"fontsize\": 20})\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9a763963a0c2e3eb3033a1766c128288cb698df5642a2fe22798efe2c630a93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
