{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "\n",
    "## Deva Kulkarni, Jared Dec, Marc Semonick, Trevor Johnson\n",
    "\n",
    "## October 2021\n",
    "\n",
    "<br>\n",
    "\n",
    "Competition Link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Goal: Predict sales price for each house (`SalePrice`). RMSE on log(pred) - log(actual) will be the evaluation metric. \n",
    "\n",
    "Inference Problem:  Given known variables about a house, accurately predict its sale price.\n",
    "\n",
    "Deliverable: Final submission dataset should contain only the two fields `ID` and `SalePrice`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import math\n",
    "import os \n",
    "\n",
    "# data manipulation/viz\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# linear modeling\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# tree modeling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#K-Fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/c/Users/drkul/desktop/mids/w207/Final_Project/w207_finalproject/models\\\\../../housing_data\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b70b49048f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# optional directory set-up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mworking_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pwd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../../housing_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../../housing_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../../housing_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/c/Users/drkul/desktop/mids/w207/Final_Project/w207_finalproject/models\\\\../../housing_data\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "# optional directory set-up\n",
    "working_dir = !pwd\n",
    "train = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"test.csv\"))\n",
    "sample = pd.read_csv(os.path.join(working_dir[0], \"../../housing_data\", \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =pd.read_csv('C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv(\"C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/test.csv\")\n",
    "sample = pd.read_csv(\"C:/Users/19258/Downloads/house-prices-advanced-regression-techniques/sample_submission.csv\")\n",
    "\n",
    "print(\"train shape: {}\".format(train.shape))\n",
    "print(\"test shape: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count number of missing values per variable:\")\n",
    "print(train.isnull().sum()[train.isnull().sum() != 0])\n",
    "\n",
    "# function to clean the missing values\n",
    "def na_clean(df):\n",
    "    \n",
    "    # some vars are just too missing so I remove the field\n",
    "    df = df.drop(columns = [\"PoolQC\", \"MiscFeature\"])\n",
    "\n",
    "    # replace some numeric vars w/ median\n",
    "    median_replace_vars = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
    "    for var in median_replace_vars:\n",
    "        df[var].fillna(df[var].median(), inplace = True)\n",
    "    \n",
    "    # replace some num vars w/ 0\n",
    "    zero_replace_vars = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\n",
    "    for var in zero_replace_vars:\n",
    "        df[var].fillna(0, inplace = True)\n",
    "    \n",
    "    # replace some cat vars w/ most freq value \n",
    "    df['MasVnrType'].fillna('None', inplace = True)\n",
    "    df['Electrical'].fillna('SBrkr', inplace = True)\n",
    "    df['MSZoning'].fillna('RL', inplace = True)\n",
    "    df['SaleType'].fillna('WD', inplace = True)\n",
    "    df['Utilities'].fillna('AllPub', inplace = True)\n",
    "    df['KitchenQual'].fillna('TA', inplace = True)\n",
    "    df['Functional'].fillna('Typ', inplace = True)\n",
    "\n",
    "    # other cat vars just put missing if there isn't a glaring most popular category\n",
    "    replace_missing_vars = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence', 'Exterior1st', \n",
    "        'Exterior2nd', 'FireplaceQu']\n",
    "    for var in replace_missing_vars:\n",
    "        df[var].fillna(\"Missing\", inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = na_clean(train)\n",
    "test = na_clean(test)\n",
    "\n",
    "# make sure there are no more missing values\n",
    "print(\"\\nNumber of missing values after running na_clean()\")\n",
    "print(\"Missing values in train: {}\".format(train.isnull().sum().sum()))\n",
    "print(\"Missing values in test: {}\".format(test.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Simplifying the data for any future model we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep adapting this formula to change how we feature engineer\n",
    "def feature_eng(df, test_data = False):\n",
    "    '''Input either the training or test data. \n",
    "    2nd arg set to True if it's the testing data. That way we ignore the final log transformation on sale price'''\n",
    "\n",
    "    # num features to just binarize b/c few houses have the feature\n",
    "    df[\"SwimmingPool\"] = df['PoolArea'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"3SsnPorch\"] = df['3SsnPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "    df[\"ScreenPorch\"] = df['ScreenPorch'].map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "    # re-factoring vars:\n",
    "    # group the irregularities into 2 factor levels\n",
    "    df['LotShape'] = df['LotShape'].map({'Reg': 'Reg', 'IR1': 'Reg', 'IR2': 'Irreg', 'IR3': 'Irreg'})\n",
    "\n",
    "    # simplifying MSSubClass because we have the year built in another feature\n",
    "    df['MSSubClass'] = df['MSSubClass'].map(lambda x: \n",
    "        \"1_story\"   if (x in (20, 30, 40, 120)) else(\n",
    "        \"1.5_story\" if (x in (45, 50, 150)) else(\n",
    "        \"2_story\"   if (x in (60, 70, 75, 160, 180, 190)) else(\n",
    "        \"split\"     if (x in (80, 85)) else(\n",
    "        \"duplex\"    if (x ==90) else(\n",
    "        \"other\"))))))\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"object\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # deciding to drop a few features for various reasons\n",
    "    vars_to_drop = [\n",
    "        \"LowQualFinSF\", # hardly any variation\n",
    "        \"LandSlope\", # not much variation\n",
    "        \"PoolArea\", # binarized above\n",
    "        \"MiscVal\", # not much variation\n",
    "        \"Utilities\", # only 1 obs in training data different from regular\n",
    "        #\"KitchenAbvGr\" # hardly any variation. But, Deva included in lm's so including it now.\n",
    "        ]\n",
    "    df.drop(columns=vars_to_drop, inplace=True) \n",
    "\n",
    "    # adding a remodeled feature\n",
    "    df['Remodeled'] = (df.YearRemodAdd-df.YearBuilt) == 0\n",
    "\n",
    "    # total inside area will be a sum of 1st and 2nd floor sq ft\n",
    "    df['Total_Inside_Area'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df.drop(columns = ['1stFlrSF', '2ndFlrSF', 'GrLivArea'], inplace = True)\n",
    "\n",
    "    # simplify the bathrooms variable\n",
    "    df['Bathrooms'] = df.BsmtFullBath + 0.5*df.BsmtHalfBath + df.FullBath + 0.5*df.HalfBath\n",
    "    df.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], inplace = True)\n",
    "\n",
    "    ### JARED EDITS\n",
    "    # minimal transformations.\n",
    "    \n",
    "    # for some vars values are 0, since ln(0) = -inf, I have to recode values of 0 to 1\n",
    "    # as ln(1) =0. \n",
    "    \n",
    "    #for x in range(len(df.BsmtFinSF1)):\n",
    "    #    if df['BsmtFinSF1'][x] ==0:\n",
    "    #        df['BsmtFinSF1'][x] = 1\n",
    "        \n",
    "    #for x in range(len(df.MasVnrArea)):\n",
    "    #    if df['MasVnrArea'][x] ==0:\n",
    "    #        df['MasVnrArea'][x] = 1\n",
    "    \n",
    "    #for x in range(len(df.GarageArea)):\n",
    "    #    if df['GarageArea'][x] ==0:\n",
    "    #        df['GarageArea'][x] = 1\n",
    "    \n",
    "    for x in range(len(df.TotalBsmtSF)):\n",
    "        if df['TotalBsmtSF'][x] ==0:\n",
    "            df['TotalBsmtSF'][x] = 1\n",
    "            \n",
    "    #for x in range(len(df.WoodDeckSF)):\n",
    "    #    if df['WoodDeckSF'][x] ==0:\n",
    "    #        df['WoodDeckSF'][x] = 1\n",
    "    \n",
    "    df.Total_Inside_Area = np.log(df.Total_Inside_Area)\n",
    "    df.TotalBsmtSF = np.log(df.TotalBsmtSF)\n",
    "    #df.GarageArea = np.log(df.GarageArea)  \n",
    "    #df.BsmtFinSF1 = np.log(df.BsmtFinSF1)\n",
    "    df.LotArea = np.log(df.LotArea)\n",
    "    #df.WoodDeckSF = np.log(df.WoodDeckSF)\n",
    "    #df.MasVnrArea = np.log(df.MasVnrArea)\n",
    "\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # get log of sale price which will be our actual response variable\n",
    "    if test_data:\n",
    "        pass \n",
    "    else:\n",
    "        df['LogSalePrice'] = np.log(df.SalePrice)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function on test and train\n",
    "train = feature_eng(train)\n",
    "test = feature_eng(test, test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prep data for a very baseline linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_df_clean(df):\n",
    "\n",
    "    lm_vars = ['LotArea', 'Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'YearRemodAdd', \n",
    "              'BsmtCond', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'YrSold', \n",
    "              'MoSold', 'Remodeled', 'Total_Inside_Area', 'Bathrooms']\n",
    "\n",
    "    df = pd.get_dummies(df[lm_vars], \n",
    "        columns = ['Street', 'Neighborhood', 'OverallQual', 'OverallCond', 'BsmtCond','KitchenQual'], \n",
    "        drop_first=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train/test data all ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = lm_df_clean(train)\n",
    "X_test = lm_df_clean(test)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline linear model is very overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decent R2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_1 = LinearRegression(fit_intercept=True)\n",
    "lr_1.fit(X_train,Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(lr_1.score(X_train,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(lr_1.score(X_test ,Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same feature set with a baseline RandomForest is also overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state = 0)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(dt.score(X_train,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(dt.score(X_test ,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic linear model with single coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplest possible model - Chose variable with highest coefficient from initial correlation matrix\n",
    "\n",
    "X_train_2 = train.drop(train.columns.difference(['OverallQual']), 1)\n",
    "X_test_2 = test.drop(test.columns.difference(['OverallQual']), 1)\n",
    "Y_train = train.SalePrice\n",
    "Y_test = sample.SalePrice\n",
    "\n",
    "lr_2 = LinearRegression(fit_intercept=True)\n",
    "lr_2.fit(X_train_2,Y_train)\n",
    "\n",
    "print(\"Fit on training data: {:.3f}\".format(lr_2.score(X_train_2,Y_train)))\n",
    "print(\"Fit on testing data: {:.3f}\".format(lr_2.score(X_test_2 ,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "train_dummies = pd.get_dummies(train)\n",
    "\n",
    "# create the train and test splits\n",
    "dev_train, dev_test = train_test_split(train_dummies, test_size=0.3, random_state=1)\n",
    "\n",
    "# specify model\n",
    "rf = RandomForestRegressor(bootstrap=True, max_features='auto')\n",
    "\n",
    "# run grid search on range of values\n",
    "grid = {\n",
    "    \"n_estimators\": np.arange(100, 1001, 100),\n",
    "    'max_depth': np.arange(10, 101, 5),\n",
    "    'min_samples_split': [2, 5, 7],\n",
    "}\n",
    "\n",
    "# put our parameters into GridSearch\n",
    "gscv = GridSearchCV(\n",
    "    estimator = rf, \n",
    "    param_grid = grid, \n",
    "    n_jobs = -1, \n",
    "    cv = 3)\n",
    "\n",
    "# run the search:\n",
    "# this took 26 min to run. \n",
    "# re-run if you want\n",
    "#results = gscv.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "# check out the best parameters\n",
    "#print(results.best_params_)\n",
    "# {'max_depth': 25, 'min_samples_split': 2, 'n_estimators': 900}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in dev_train:\n",
    "    for x in dev_train[i]:\n",
    "        if x ==np.inf:\n",
    "            print(i)\n",
    "        elif x ==-np.inf:\n",
    "            print(i)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, run model on best parameters\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators = 900,\n",
    "    max_depth = 25,\n",
    "    max_features = 'auto',\n",
    "    min_samples_split = 2,  \n",
    "    bootstrap = True, \n",
    "    )\n",
    "\n",
    "# fit the model\n",
    "rf_reg.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "# evaluate it's performance on the unforseen testing set\n",
    "yhat = rf_reg.predict(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']))\n",
    "resids = yhat - dev_test.LogSalePrice\n",
    "\n",
    "# evaluate rmse and mae on the testing data\n",
    "print(\"MAE: \" + str(round(np.mean(resids.apply(lambda x: math.fabs(x))),4)))\n",
    "print(\"RMSE: \" + str(round(np.mean(resids**2)**.5,4)))\n",
    "# MAE: 0.0999\n",
    "# RMSE: 0.154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train\n",
    "rf_train_score = rf_reg.score(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "print(\"Score on train: {:.3f}\".format(rf_train_score))\n",
    "\n",
    "# score on test/dev\n",
    "rf_dev_score = rf_reg.score(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_test.LogSalePrice)\n",
    "print(\"Score on test: {:.3f}\".format(rf_dev_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable importance plot\n",
    "var_imp = pd.DataFrame({\n",
    "    'feature': dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']).columns,\n",
    "    'importance': rf_reg.feature_importances_\n",
    "})\n",
    "var_imp.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 7))\n",
    "topn = 20\n",
    "ax.barh(var_imp.head(topn).feature, var_imp.head(topn).importance)\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_title(\"Feature Importance Plot: Top {}\".format(topn))\n",
    "ax.invert_yaxis()\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]:\n",
    "\n",
    "    # iterate over every vbalkue of alpha in the list, fit lasso regression\n",
    "    model = Lasso(alpha = i)\n",
    "\n",
    "    # copying Trevor's code, delete certain variables that don't matter\n",
    "    model.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "    # run 10-fold Cross validation to improve fit\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate scores\n",
    "    scores = cross_val_score(model, dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    # calculate MAE, accuracy.\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model.score(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_test.LogSalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/whats-the-difference-between-linear-regression-lasso-ridge-and-elasticnet-8f997c60cf29\n",
    "\n",
    "# set asige log_sale_price in separate vectors\n",
    "train_log_saleprice = dev_train.LogSalePrice\n",
    "test_log_saleprice = dev_test.LogSalePrice\n",
    "\n",
    "# drop unused variables\n",
    "dev_train2 = dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id'])\n",
    "dev_test2 = dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id'])\n",
    "\n",
    "# standardize all values of all variables in data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(dev_train2)\n",
    "dev_train2 = scaler.transform(dev_train2)\n",
    "\n",
    "scaler.fit(dev_test2)\n",
    "dev_test2 = scaler.transform(dev_test2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]:\n",
    "    \n",
    "    # run and fit lasso regression again, this time with completely standardized data\n",
    "    model = Lasso(alpha = i)\n",
    "\n",
    "    model.fit(dev_train2, train_log_saleprice)\n",
    "\n",
    "    # 10-fold cross validation to improve fit\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate scores and accuracy\n",
    "    scores = cross_val_score(model, dev_train2, train_log_saleprice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model.score(dev_test2, test_log_saleprice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 25.0, 50.0, 75.0, 100.0]:\n",
    "\n",
    "    # calculate ridge regression with completely standardized data\n",
    "    model2 = Ridge(alpha = i)\n",
    "\n",
    "    model2.fit(dev_train2, train_log_saleprice)\n",
    "\n",
    "    # run 10-fold cross validation\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate score\n",
    "    scores = cross_val_score(model, dev_train2, train_log_saleprice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    # calculate MAE and Accuracy\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model2.score(dev_test2, test_log_saleprice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]:\n",
    "\n",
    "    # calculate Elastic Net with completely standardized data\n",
    "    model3 = ElasticNet(alpha = i)\n",
    "\n",
    "    model3.fit(dev_train2, train_log_saleprice)\n",
    "\n",
    "    # 10-fold cross-validation to improve fit\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate scores, MAE, and accuracy\n",
    "    scores = cross_val_score(model, dev_train2, train_log_saleprice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model3.score(dev_test2, test_log_saleprice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]:\n",
    "\n",
    "    # Ridge Regression with minimally transofrmed data\n",
    "    model4 = Ridge(alpha = i)\n",
    "\n",
    "    model4.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "    # 10-fold cross-validation\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate scores, MAE, and Accuracy\n",
    "    scores = cross_val_score(model, dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model4.score(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_test.LogSalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]:\n",
    "\n",
    "    # calculate elastic net with minimally transformed data\n",
    "    model5 = ElasticNet(alpha = i)\n",
    "\n",
    "    model5.fit(dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice)\n",
    "\n",
    "    # run 10-fold cross-validation\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # calculate scores, MAE, and accuracy\n",
    "    scores = cross_val_score(model, dev_train.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_train.LogSalePrice, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    scores = absolute(scores)\n",
    "\n",
    "    print(\"Lasso alpha is equal to: \", i)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "    print('Accuracy:', model5.score(dev_test.drop(columns = [\"SalePrice\", 'LogSalePrice', 'Id']), dev_test.LogSalePrice))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9a763963a0c2e3eb3033a1766c128288cb698df5642a2fe22798efe2c630a93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
